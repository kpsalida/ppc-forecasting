{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')  # Set whitegrid style from Seaborn\n",
    "#import mplfinance as mpf\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "import plotly.express as px\n",
    "import matplotlib.ticker as tkr\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "#Keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# keras keras\n",
    "\n",
    "from tensorflow.python.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential, Model, load_model, save_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D, Embedding\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "from tensorflow.keras.layers import Reshape, Dropout, Dense,Multiply, Dot, Concatenate,Embedding\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "#from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, SimpleRNN, Dropout, GRU, Bidirectional,Dense\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, LSTM,Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Time-series\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# Modelling and Forecasting\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, root_mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#KERAS GRID Search\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.activations import swish\n",
    "#\n",
    "import math\n",
    "#\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score,mean_squared_error\n",
    "#\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "#\n",
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "#iimport pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS\n",
    "#Function to separate the DATA for LSTM\n",
    "def createXY(dataset, n_lags):\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "\n",
    "    for i in range(n_lags, len(dataset)):\n",
    "            dataX.append(dataset[i - n_lags:i, 0:dataset.shape[1]])\n",
    "            dataY.append(dataset[i,0])\n",
    "\n",
    "    return np.array(dataX),np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Various Functions\n",
    "#def mean_error(y_true, y_pred):\n",
    "#   error = y_true - y_pred\n",
    "#   return np.mean(error) \n",
    "\n",
    "def mean_error(y_true, y_pred):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    error_sum = 0\n",
    "    for pred, true in zip(y_pred, y_true):\n",
    "        error_sum += true + (-pred)\n",
    "    return error_sum / len(y_true)\n",
    "\n",
    "\n",
    "def mean_directional_accuracy(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Directional Accuracy (MDA) for two time series.\n",
    "    MDA measures the percentage of times the forecast and the actual values move in the same direction (i.e., both increase or both decrease).\n",
    "    \n",
    "    Parameters:\n",
    "    actual (array-like): The actual values for the time series.\n",
    "    predicted (array-like): The predicted values for the time series.\n",
    "    \n",
    "    Returns:\n",
    "    float: The MDA value.\n",
    "    \"\"\"\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Check if inputs are of the same length and non-empty\n",
    "    if len(actual) != len(predicted):\n",
    "        raise ValueError(\"The lengths of actual and predicted must be the same.\")\n",
    "    if len(actual) < 2:\n",
    "        raise ValueError(\"The length of the time series must be at least 2 to compute MDA.\")\n",
    "    \n",
    "    # Calculate the signs of the differences between consecutive values\n",
    "    actual_diff = np.diff(actual)\n",
    "    actual_signs = np.sign(actual_diff)\n",
    "    predicted_diff = np.diff(predicted)\n",
    "    predicted_signs = np.sign(predicted_diff)\n",
    "    \n",
    "    # Count the number of times the signs are the same\n",
    "    num_correct = np.sum(actual_signs == predicted_signs)\n",
    "    \n",
    "    # Calculate the MDA value\n",
    "    mda = num_correct / (len(actual) - 1)\n",
    "    \n",
    "    return mda\n",
    "#\n",
    "def directional_accuracy(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculates the Directional Accuracy (DA) for two time series.\n",
    "    \n",
    "    Parameters:\n",
    "    actual (array-like): The actual values for the time series.\n",
    "    predicted (array-like): The predicted values for the time series.\n",
    "    \n",
    "    Returns:\n",
    "    float: The DA value.\n",
    "    \"\"\"\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Check if inputs are of the same length and non-empty\n",
    "    if len(actual) != len(predicted):\n",
    "        raise ValueError(\"The lengths of actual and predicted must be the same.\")\n",
    "    if len(actual) < 2:\n",
    "        raise ValueError(\"The length of the time series must be at least 2 to compute DA.\")\n",
    "    \n",
    "    # Calculate differences between consecutive values\n",
    "    actual_diff = np.diff(actual)\n",
    "    predicted_diff = np.diff(predicted)\n",
    "    \n",
    "    # Calculate the indicator function for directional accuracy\n",
    "    indicator = (predicted_diff * actual_diff) > 0\n",
    "    \n",
    "    # Count the number of correct directions\n",
    "    num_correct_directions = np.sum(indicator)\n",
    "    \n",
    "    # Calculate the DA value\n",
    "    da = num_correct_directions / (len(actual) - 1)\n",
    "    \n",
    "    return da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluation(y_test, y_pred, data_test,title):\n",
    "    # Plot the actual vs predicted values\n",
    "    fig = plt.figure(figsize=(8, 3))\n",
    "    plt.plot(y_test, color='red', label='Actual')\n",
    "    plt.plot(y_pred, color='blue', label='Predicted')\n",
    "    #plt.xticks(np.arange(0, len(data_test), 100), data_test.index[np.arange(0, len(data_test), 100)], rotation=45)\n",
    "    plt.title(f\"Predictions vs Actual Data using {title}\")\n",
    "    #plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('EUR')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Assuming data_test.index is a datetime index\n",
    "    plt.xticks(np.arange(0, len(data_test), 100), data_test.index[np.arange(0, len(data_test), 100)].strftime('%Y-%m-%d'), rotation=0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    #mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    me = mean_error(y_test, y_pred)\n",
    "    mda = mean_directional_accuracy(y_test, y_pred)\n",
    "    da  = directional_accuracy(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Round metrics for display\n",
    "   # mape = np.round(mape, 3)\n",
    "    mae = np.round(mae, 3)\n",
    "    mse = np.round(mse, 3)\n",
    "    rmse = np.round(rmse, 3)\n",
    "    me = np.round(me, 3)\n",
    "    mda = np.round(mda, 3)\n",
    "    da= np.round(da, 3)\n",
    "    r2 = np.round(r2, 3)\n",
    "\n",
    "    # Print metrics\n",
    "    print('MAE: %f  --  MSE: %f  --  RMSE: %f  --  ME: %f  --  MDA: %f  --  DA: %f  --  R2: %f' % \n",
    "          (mae, mse, rmse, me, mda, da, r2))\n",
    "\n",
    "    return mae, mse, rmse, me, mda, da, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINED MODEL AND SCALER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model_gru = load_model('GRU_MODEL_KERAS.keras')\n",
    "#Load the scaler\n",
    "scaler = pickle.load(open('SCALERGRU.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA WITH HOLIDAYS DAYS WEEKS MONTH IN SIN/COS. NEED TO USE NOTEBOOK CreateFeatureDataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the past 48 Hours_ These are the 48 LAGS to put into the model to find the next hour. LAST SEQUENCE of DATA\n",
    "df_24 = pd.read_csv('PAST_24HOURS_SIN.csv')\n",
    "df_24.drop(columns=['Unnamed: 0'],inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime_from</th>\n",
       "      <th>price</th>\n",
       "      <th>price1</th>\n",
       "      <th>price2</th>\n",
       "      <th>price3</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>dayofweek_sin</th>\n",
       "      <th>dayofweek_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-01 00:00:00</td>\n",
       "      <td>86.07</td>\n",
       "      <td>92.98</td>\n",
       "      <td>102.76</td>\n",
       "      <td>84.455</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-01 01:00:00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>90.37</td>\n",
       "      <td>101.96</td>\n",
       "      <td>84.156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-01 02:00:00</td>\n",
       "      <td>92.53</td>\n",
       "      <td>93.08</td>\n",
       "      <td>101.86</td>\n",
       "      <td>84.687</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-01 03:00:00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>96.77</td>\n",
       "      <td>106.27</td>\n",
       "      <td>91.973</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-01 04:00:00</td>\n",
       "      <td>140.25</td>\n",
       "      <td>110.12</td>\n",
       "      <td>118.50</td>\n",
       "      <td>109.203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         datetime_from   price  price1  price2   price3  is_weekend  \\\n",
       "0  2024-07-01 00:00:00   86.07   92.98  102.76   84.455           0   \n",
       "1  2024-07-01 01:00:00   90.00   90.37  101.96   84.156           0   \n",
       "2  2024-07-01 02:00:00   92.53   93.08  101.86   84.687           0   \n",
       "3  2024-07-01 03:00:00  104.00   96.77  106.27   91.973           0   \n",
       "4  2024-07-01 04:00:00  140.25  110.12  118.50  109.203           0   \n",
       "\n",
       "   is_holiday  dayofweek_sin  dayofweek_cos     month_sin  month_cos  \\\n",
       "0           0            0.0            1.0  1.224647e-16       -1.0   \n",
       "1           0            0.0            1.0  1.224647e-16       -1.0   \n",
       "2           0            0.0            1.0  1.224647e-16       -1.0   \n",
       "3           0            0.0            1.0  1.224647e-16       -1.0   \n",
       "4           0            0.0            1.0  1.224647e-16       -1.0   \n",
       "\n",
       "   hour_sin  hour_cos  \n",
       "0  0.000000  1.000000  \n",
       "1  0.258819  0.965926  \n",
       "2  0.500000  0.866025  \n",
       "3  0.707107  0.707107  \n",
       "4  0.866025  0.500000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_24.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD the data for the next day with the Forecaster Predictions \n",
    "df_last_day = pd.read_csv('NEXTDAY24_FORECASTERS_SIN.csv')\n",
    "df_last_day.drop(columns=['Unnamed: 0','price'],inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_24['datetime_from'] = pd.to_datetime(df_24['datetime_from'])\n",
    "df_24 = df_24.set_index('datetime_from')\n",
    "df_24 = df_24.asfreq('1H')\n",
    "df_24 = df_24.sort_index()\n",
    "(df_24.index == pd.date_range(start=df_24.index.min(),end=df_24.index.max(),freq=df_24.index.freq)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_last_day['datetime_from'] = pd.to_datetime(df_last_day['datetime_from'])\n",
    "df_last_day = df_last_day.set_index('datetime_from')\n",
    "df_last_day = df_last_day.asfreq('1H')\n",
    "df_last_day = df_last_day.sort_index()\n",
    "(df_last_day.index == pd.date_range(start=df_last_day.index.min(),end=df_last_day.index.max(),freq=df_last_day.index.freq)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape should be (24,12)\n",
    "df_24.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape should be (24,11)\n",
    "df_last_day.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINE PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "lag = 24\n",
    "num_features = df_24.shape[1]\n",
    "next_predictions = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCALE THE DATA of PAST OBSERVATIONS DF_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scale the DATA with IMPORTED SCALER\n",
    "\n",
    "data_scaled = scaler.transform(df_24.values)\n",
    "\n",
    "#Need this to be (24,12)\n",
    "data_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT NEXT 24 HOURS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Expects an input of (1, 24, 12) to make a prediction of (1,)\n",
    "#RESHAPE THE DATA (1, lag, num_features)\n",
    "last_sequence = data_scaled.reshape((1, lag, num_features))  # (1, 24, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n"
     ]
    }
   ],
   "source": [
    "# Initialize the predictions dictionary\n",
    "predictions = {\n",
    "    'pred': [],\n",
    "    'price1': [],\n",
    "    'price2': [],\n",
    "    'price3': []\n",
    "}\n",
    "\n",
    "# Predict 24 hours ahead (next_predictions)\n",
    "for i in range(next_predictions):\n",
    "    # Predict the next hour\n",
    "    next_prediction = model_gru.predict(last_sequence)\n",
    "    \n",
    "    # Inverse scale the prediction for the first feature\n",
    "    prediction_reverselyscaled = scaler.inverse_transform(np.repeat(next_prediction, num_features, axis=-1))[:, 0]\n",
    "    \n",
    "    # Get the next timestamp\n",
    "    next_timestamp = df_24.index[-1] + pd.Timedelta(hours=i + 1)\n",
    "    #print('next_timestamp: ',next_timestamp)\n",
    "    \n",
    "    # Extract the next feature values from df_last_day for the next timestamp\n",
    "    if next_timestamp in df_last_day.index:\n",
    "        next_features = df_last_day.loc[next_timestamp, ['price1', 'price2', 'price3', 'is_weekend', 'is_holiday', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos', 'hour_sin', 'hour_cos']].values\n",
    "    else:\n",
    "        next_features = np.zeros(num_features - 1)  # Default to zeros if timestamp is out of range\n",
    "        print('Timestamp not in Next Day !!!')\n",
    "    \n",
    "    # Combine the predicted value with the next feature values\n",
    "    new_row = np.concatenate(([prediction_reverselyscaled[0]], next_features))\n",
    "    # Scale the new row\n",
    "    new_row_scaled = scaler.transform(new_row.reshape(1, -1)).flatten()\n",
    "\n",
    "    # Update the last_sequence with the new prediction and feature values\n",
    "    next_input = np.append(last_sequence[0, 1:, :], [new_row_scaled], axis=0)  # Shift and append the prediction\n",
    "    last_sequence = next_input.reshape((1, lag, num_features))\n",
    "    \n",
    "    # Update the predictions dictionary\n",
    "    predictions['pred'].append(prediction_reverselyscaled[0])\n",
    "    if next_timestamp in df_last_day.index:\n",
    "        predictions['price1'].append(df_last_day.loc[next_timestamp, 'price1'])\n",
    "        predictions['price2'].append(df_last_day.loc[next_timestamp, 'price2'])\n",
    "        predictions['price3'].append(df_last_day.loc[next_timestamp, 'price3'])\n",
    "    else:\n",
    "        predictions['price1'].append(None) # Append None if the timestamp is out of range\n",
    "        predictions['price2'].append(None)\n",
    "        predictions['price3'].append(None)\n",
    "    \n",
    "    # Debugging: Print the current prediction and last sequence\n",
    "    #print(f\"Prediction {i+1}: {predictions['pred'][-1]}\")\n",
    "    #print(f\"Actual {i+1}: {predictions['actual'][-1]}\")\n",
    "    #print(f\"last_sequence before update: {last_sequence[0, -1, :]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-hour ahead predictions:\n",
      "Predicted Prices:  [104.92036, 102.64738, 101.78009, 106.29398, 117.32009, 134.73607, 143.774, 128.66585, 114.02026, 102.88171, 95.32001, 92.38086, 93.3325, 95.7201, 109.326866, 123.21036, 149.39859, 206.14989, 265.15945, 219.73792, 157.10446, 129.66829, 118.57099, 107.275665]\n",
      "Price1:  [121.26, 114.02, 115.74, 120.53, 139.18, 141.21, 132.88, 121.5, 110.53, 105.25, 106.19, 109.59, 109.01, 123.32, 138.23, 185.26, 251.67, 290.56, 287.71, 226.52, 163.51, 134.43, 96.33, 93.89]\n",
      "Price2:  [104.18, 103.18, 102.94, 106.32, 119.03, 125.14, 113.0, 95.66, 86.53, 81.18, 81.58, 87.06, 92.54, 100.39, 116.62, 114.67, 155.64, 239.54, 278.15, 209.92, 137.36, 115.99, 110.49, 101.37]\n",
      "Price3:  [95.612, 93.174, 93.678, 104.669, 122.26, 149.214, 128.035, 105.012, 87.162, 78.436, 73.386, 73.735, 75.004, 92.217, 100.722, 110.045, 138.43, 219.046, 278.24, 209.134, 137.654, 116.154, 97.723, 97.265]\n",
      "None None None None\n"
     ]
    }
   ],
   "source": [
    "# Print the 24-hour ahead predictions\n",
    "print(\"24-hour ahead predictions:\")\n",
    "print(print('Predicted Prices: ',predictions['pred']),print('Price1: ',predictions['price1']),print('Price2: ',predictions['price2']), print('Price3: ',predictions['price3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
